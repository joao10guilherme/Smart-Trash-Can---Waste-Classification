{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed350afd",
   "metadata": {},
   "source": [
    "Working on Second version of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a08e1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets, models\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9068bb57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.13), please consider upgrading to the latest version (0.4.1).\n",
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/sumn2u/garbage-classification-v2?dataset_version_number=10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782M/782M [00:09<00:00, 82.1MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /root/.cache/kagglehub/datasets/sumn2u/garbage-classification-v2/versions/10\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "#Download latast version\n",
    "path = kagglehub.dataset_download(\"sumn2u/garbage-classification-v2\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "515d8d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['trash', 'battery', 'metal', 'clothes', 'biological', 'paper', 'cardboard', 'shoes', 'plastic', 'glass']\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"/root/.cache/kagglehub/datasets/sumn2u/garbage-classification-v2/versions/10\"\n",
    "\n",
    "classes = os.listdir(data_dir)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f98710ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0206f636",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ImageNet stats (MobileNetV3 was pretrained on ImageNet)\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "input_size = 224\n",
    "\n",
    "#Define transforms (resize small images, normalize)\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(input_size, scale=(0.8,1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(input_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a4196c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create datasets with different transforms\n",
    "train_dataset_full = datasets.ImageFolder(\n",
    "    data_dir,\n",
    "    transform=train_transforms\n",
    ")\n",
    "\n",
    "val_dataset_full = datasets.ImageFolder(\n",
    "    data_dir,\n",
    "    transform=val_transforms\n",
    ")\n",
    "\n",
    "#Split into train and val (80% train, 20% val)\n",
    "val_percent = 0.2\n",
    "val_size = int(len(train_dataset_full) * val_percent)\n",
    "train_size = len(train_dataset_full) - val_size\n",
    "\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "\n",
    "train_dataset, _ = random_split(\n",
    "    train_dataset_full,\n",
    "    [train_size, val_size],\n",
    "    generator=generator\n",
    ")\n",
    "\n",
    "_, val_dataset = random_split(\n",
    "    val_dataset_full,\n",
    "    [train_size, val_size],\n",
    "    generator=generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77ecae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_dataset = datasets.ImageFolder(data_dir)\n",
    "\n",
    "# val_percent = 0.2\n",
    "# val_size = int(len(full_dataset) * val_percent)\n",
    "# train_size = len(full_dataset) - val_size\n",
    "\n",
    "# generator = torch.Generator().manual_seed(42)\n",
    "\n",
    "# train_dataset, val_dataset = random_split(\n",
    "#     full_dataset,\n",
    "#     [train_size, val_size],\n",
    "#     generator=generator\n",
    "# )\n",
    "\n",
    "# class TransformSubset(torch.utils.data.Dataset):\n",
    "\n",
    "#     def __init__(self, subset, transform):\n",
    "#         self.subset = subset\n",
    "#         self.transform = transform\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.subset)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         x, y = self.subset[idx]\n",
    "#         if self.transform:\n",
    "#             x = self.transform(x)\n",
    "#         return x, y\n",
    "    \n",
    "# train_dataset = TransformSubset(train_dataset, train_transforms)\n",
    "# val_dataset = TransformSubset(val_dataset, val_transforms)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "237d7f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_dataset: ['battery', 'biological', 'cardboard', 'clothes', 'glass', 'metal', 'paper', 'plastic', 'shoes', 'trash']\n",
      "Val_dataset: ['battery', 'biological', 'cardboard', 'clothes', 'glass', 'metal', 'paper', 'plastic', 'shoes', 'trash']\n",
      "Number of training classes: 10\n",
      "Number of validation classes: 10\n"
     ]
    }
   ],
   "source": [
    "#DataLoaders\n",
    "num_workers = os.cpu_count() // 2\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "train_classes = train_dataset_full.classes\n",
    "print(f\"Train_dataset: {train_classes}\")\n",
    "val_classes = val_dataset_full.classes\n",
    "print(f\"Val_dataset: {val_classes}\")\n",
    "\n",
    "#Get class count\n",
    "num_train_classes = len(train_dataset_full.classes)\n",
    "num_val_classes = len(val_dataset_full.classes)\n",
    "print(f\"Number of training classes: {num_train_classes}\")\n",
    "print(f\"Number of validation classes: {num_val_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ff204e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/mobilenet_v3_small-047dcff4.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v3_small-047dcff4.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.83M/9.83M [00:00<00:00, 85.0MB/s]\n"
     ]
    }
   ],
   "source": [
    "#Load pretrained MobileNetV3-Small\n",
    "model = models.mobilenet_v3_small(weights='DEFAULT')\n",
    "\n",
    "#Freeze all layers (no gradients = no training)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "#Unfreeze last layer\n",
    "for param in model.features[-1:].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "#Replace the classifier head with a custom classifier\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Linear(model.classifier[0].in_features, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(256, num_train_classes)\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "#Loss and optimizer setup\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d31aa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, num_epochs=10):\n",
    "\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            #Ass batch-level log\n",
    "            if (batch_idx + 1) % 10 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], \"\n",
    "                      f\"Loss: {loss.item():.4f}\")\n",
    "            \n",
    "        train_loss = running_loss / total\n",
    "        train_acc = 100 * (correct / total)\n",
    "\n",
    "        #Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        # Per-class stats\n",
    "        class_correct = torch.zeros(num_train_classes, device=device)\n",
    "        class_total = torch.zeros(num_train_classes, device=device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "                all_preds.append(predicted.cpu())\n",
    "                all_labels.append(labels.cpu())\n",
    "\n",
    "                for i in range(labels.size(0)):\n",
    "                    label = labels[i]\n",
    "                    class_total[label] += 1\n",
    "                    if predicted[i] == label:\n",
    "                        class_correct[label] += 1\n",
    "\n",
    "        val_loss /= val_total\n",
    "        val_acc = 100 * (val_correct / val_total)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Acc: {train_acc:.2f}% | \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%\")\n",
    "        \n",
    "        # Per-class accuracy\n",
    "        for cls in range(num_train_classes):\n",
    "            cls_mask = labels == cls\n",
    "            class_total[cls] += cls_mask.sum()\n",
    "            class_correct[cls] += (predicted[cls_mask] == cls).sum()\n",
    "            acc = 100 * (class_correct[cls] / class_total[cls])\n",
    "            print(f\"    Class {train_classes[cls]}: {acc:.2f}%\")\n",
    "\n",
    "        all_preds = torch.cat(all_preds).numpy()\n",
    "        all_labels = torch.cat(all_labels).numpy()\n",
    "\n",
    "        print(classification_report(\n",
    "            all_labels,\n",
    "            all_preds,\n",
    "            target_names=train_classes\n",
    "        ))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a7909976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Batch [10/506], Loss: 0.2258\n",
      "Epoch [1/10], Batch [20/506], Loss: 0.3744\n",
      "Epoch [1/10], Batch [30/506], Loss: 0.4937\n",
      "Epoch [1/10], Batch [40/506], Loss: 0.2834\n",
      "Epoch [1/10], Batch [50/506], Loss: 0.4103\n",
      "Epoch [1/10], Batch [60/506], Loss: 0.1328\n",
      "Epoch [1/10], Batch [70/506], Loss: 0.1536\n",
      "Epoch [1/10], Batch [80/506], Loss: 0.4631\n",
      "Epoch [1/10], Batch [90/506], Loss: 0.3864\n",
      "Epoch [1/10], Batch [100/506], Loss: 0.5333\n",
      "Epoch [1/10], Batch [110/506], Loss: 0.4808\n",
      "Epoch [1/10], Batch [120/506], Loss: 0.2478\n",
      "Epoch [1/10], Batch [130/506], Loss: 0.3841\n",
      "Epoch [1/10], Batch [140/506], Loss: 0.3477\n",
      "Epoch [1/10], Batch [150/506], Loss: 0.6945\n",
      "Epoch [1/10], Batch [160/506], Loss: 0.4671\n",
      "Epoch [1/10], Batch [170/506], Loss: 0.3762\n",
      "Epoch [1/10], Batch [180/506], Loss: 0.3721\n",
      "Epoch [1/10], Batch [190/506], Loss: 0.4370\n",
      "Epoch [1/10], Batch [200/506], Loss: 0.3630\n",
      "Epoch [1/10], Batch [210/506], Loss: 0.4151\n",
      "Epoch [1/10], Batch [220/506], Loss: 0.3071\n",
      "Epoch [1/10], Batch [230/506], Loss: 0.6970\n",
      "Epoch [1/10], Batch [240/506], Loss: 0.3640\n",
      "Epoch [1/10], Batch [250/506], Loss: 0.2716\n",
      "Epoch [1/10], Batch [260/506], Loss: 0.5094\n",
      "Epoch [1/10], Batch [270/506], Loss: 0.3787\n",
      "Epoch [1/10], Batch [280/506], Loss: 0.4172\n",
      "Epoch [1/10], Batch [290/506], Loss: 0.2656\n",
      "Epoch [1/10], Batch [300/506], Loss: 0.4132\n",
      "Epoch [1/10], Batch [310/506], Loss: 0.4553\n",
      "Epoch [1/10], Batch [320/506], Loss: 0.2050\n",
      "Epoch [1/10], Batch [330/506], Loss: 0.4014\n",
      "Epoch [1/10], Batch [340/506], Loss: 0.2191\n",
      "Epoch [1/10], Batch [350/506], Loss: 0.5347\n",
      "Epoch [1/10], Batch [360/506], Loss: 0.1436\n",
      "Epoch [1/10], Batch [370/506], Loss: 0.3044\n",
      "Epoch [1/10], Batch [380/506], Loss: 0.1287\n",
      "Epoch [1/10], Batch [390/506], Loss: 0.2346\n",
      "Epoch [1/10], Batch [400/506], Loss: 0.3657\n",
      "Epoch [1/10], Batch [410/506], Loss: 0.2677\n",
      "Epoch [1/10], Batch [420/506], Loss: 0.3688\n",
      "Epoch [1/10], Batch [430/506], Loss: 0.4088\n",
      "Epoch [1/10], Batch [440/506], Loss: 0.4133\n",
      "Epoch [1/10], Batch [450/506], Loss: 0.5070\n",
      "Epoch [1/10], Batch [460/506], Loss: 0.2970\n",
      "Epoch [1/10], Batch [470/506], Loss: 0.8546\n",
      "Epoch [1/10], Batch [480/506], Loss: 0.0817\n",
      "Epoch [1/10], Batch [490/506], Loss: 0.0704\n",
      "Epoch [1/10], Batch [500/506], Loss: 0.3429\n",
      "Epoch 1/10 | Train Loss: 0.3671, Acc: 88.00% | Val Loss: 0.3103, Acc: 89.81%\n",
      "    Class battery: 96.41%\n",
      "    Class biological: 96.28%\n",
      "    Class cardboard: 84.49%\n",
      "    Class clothes: 97.37%\n",
      "    Class glass: 91.83%\n",
      "    Class metal: 81.94%\n",
      "    Class paper: 86.15%\n",
      "    Class plastic: 78.24%\n",
      "    Class shoes: 95.83%\n",
      "    Class trash: 70.24%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     battery       0.88      0.96      0.92       166\n",
      "  biological       0.89      0.96      0.92       188\n",
      "   cardboard       0.90      0.85      0.87       372\n",
      "     clothes       0.97      0.97      0.97      1099\n",
      "       glass       0.88      0.92      0.90       649\n",
      "       metal       0.77      0.82      0.80       226\n",
      "       paper       0.82      0.86      0.84       323\n",
      "     plastic       0.87      0.78      0.82       454\n",
      "       shoes       0.92      0.96      0.94       360\n",
      "       trash       0.87      0.70      0.78       205\n",
      "\n",
      "    accuracy                           0.90      4042\n",
      "   macro avg       0.88      0.88      0.88      4042\n",
      "weighted avg       0.90      0.90      0.90      4042\n",
      "\n",
      "Epoch [2/10], Batch [10/506], Loss: 0.3249\n",
      "Epoch [2/10], Batch [20/506], Loss: 0.5246\n",
      "Epoch [2/10], Batch [30/506], Loss: 0.2730\n",
      "Epoch [2/10], Batch [40/506], Loss: 0.2177\n",
      "Epoch [2/10], Batch [50/506], Loss: 0.2284\n",
      "Epoch [2/10], Batch [60/506], Loss: 0.3253\n",
      "Epoch [2/10], Batch [70/506], Loss: 0.3800\n",
      "Epoch [2/10], Batch [80/506], Loss: 0.2930\n",
      "Epoch [2/10], Batch [90/506], Loss: 0.4997\n",
      "Epoch [2/10], Batch [100/506], Loss: 0.1488\n",
      "Epoch [2/10], Batch [110/506], Loss: 0.5890\n",
      "Epoch [2/10], Batch [120/506], Loss: 0.2880\n",
      "Epoch [2/10], Batch [130/506], Loss: 0.3860\n",
      "Epoch [2/10], Batch [140/506], Loss: 0.4340\n",
      "Epoch [2/10], Batch [150/506], Loss: 0.3139\n",
      "Epoch [2/10], Batch [160/506], Loss: 0.5427\n",
      "Epoch [2/10], Batch [170/506], Loss: 0.2793\n",
      "Epoch [2/10], Batch [180/506], Loss: 0.3776\n",
      "Epoch [2/10], Batch [190/506], Loss: 0.2570\n",
      "Epoch [2/10], Batch [200/506], Loss: 0.5098\n",
      "Epoch [2/10], Batch [210/506], Loss: 0.0835\n",
      "Epoch [2/10], Batch [220/506], Loss: 0.3691\n",
      "Epoch [2/10], Batch [230/506], Loss: 0.2526\n",
      "Epoch [2/10], Batch [240/506], Loss: 0.2131\n",
      "Epoch [2/10], Batch [250/506], Loss: 0.4105\n",
      "Epoch [2/10], Batch [260/506], Loss: 0.0894\n",
      "Epoch [2/10], Batch [270/506], Loss: 0.4341\n",
      "Epoch [2/10], Batch [280/506], Loss: 0.3531\n",
      "Epoch [2/10], Batch [290/506], Loss: 0.3012\n",
      "Epoch [2/10], Batch [300/506], Loss: 0.3616\n",
      "Epoch [2/10], Batch [310/506], Loss: 0.2922\n",
      "Epoch [2/10], Batch [320/506], Loss: 0.3148\n",
      "Epoch [2/10], Batch [330/506], Loss: 0.1868\n",
      "Epoch [2/10], Batch [340/506], Loss: 0.3668\n",
      "Epoch [2/10], Batch [350/506], Loss: 0.2979\n",
      "Epoch [2/10], Batch [360/506], Loss: 0.1925\n",
      "Epoch [2/10], Batch [370/506], Loss: 0.3681\n",
      "Epoch [2/10], Batch [380/506], Loss: 0.4950\n",
      "Epoch [2/10], Batch [390/506], Loss: 0.5491\n",
      "Epoch [2/10], Batch [400/506], Loss: 0.3872\n",
      "Epoch [2/10], Batch [410/506], Loss: 0.5418\n",
      "Epoch [2/10], Batch [420/506], Loss: 0.1502\n",
      "Epoch [2/10], Batch [430/506], Loss: 0.3241\n",
      "Epoch [2/10], Batch [440/506], Loss: 0.2413\n",
      "Epoch [2/10], Batch [450/506], Loss: 0.2345\n",
      "Epoch [2/10], Batch [460/506], Loss: 0.3477\n",
      "Epoch [2/10], Batch [470/506], Loss: 0.5101\n",
      "Epoch [2/10], Batch [480/506], Loss: 0.2167\n",
      "Epoch [2/10], Batch [490/506], Loss: 0.3127\n",
      "Epoch [2/10], Batch [500/506], Loss: 0.2041\n",
      "Epoch 2/10 | Train Loss: 0.3593, Acc: 88.05% | Val Loss: 0.3083, Acc: 89.71%\n",
      "    Class battery: 94.01%\n",
      "    Class biological: 96.81%\n",
      "    Class cardboard: 85.03%\n",
      "    Class clothes: 97.10%\n",
      "    Class glass: 91.83%\n",
      "    Class metal: 83.70%\n",
      "    Class paper: 88.00%\n",
      "    Class plastic: 76.26%\n",
      "    Class shoes: 95.56%\n",
      "    Class trash: 69.76%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     battery       0.93      0.94      0.93       166\n",
      "  biological       0.90      0.97      0.93       188\n",
      "   cardboard       0.90      0.85      0.88       372\n",
      "     clothes       0.97      0.97      0.97      1099\n",
      "       glass       0.86      0.92      0.89       649\n",
      "       metal       0.77      0.84      0.80       226\n",
      "       paper       0.80      0.88      0.84       323\n",
      "     plastic       0.89      0.76      0.82       454\n",
      "       shoes       0.91      0.96      0.93       360\n",
      "       trash       0.89      0.70      0.78       205\n",
      "\n",
      "    accuracy                           0.90      4042\n",
      "   macro avg       0.88      0.88      0.88      4042\n",
      "weighted avg       0.90      0.90      0.90      4042\n",
      "\n",
      "Epoch [3/10], Batch [10/506], Loss: 0.2679\n",
      "Epoch [3/10], Batch [20/506], Loss: 0.1925\n",
      "Epoch [3/10], Batch [30/506], Loss: 0.3566\n",
      "Epoch [3/10], Batch [40/506], Loss: 0.1972\n",
      "Epoch [3/10], Batch [50/506], Loss: 0.2400\n",
      "Epoch [3/10], Batch [60/506], Loss: 0.3445\n",
      "Epoch [3/10], Batch [70/506], Loss: 0.4652\n",
      "Epoch [3/10], Batch [80/506], Loss: 0.4210\n",
      "Epoch [3/10], Batch [90/506], Loss: 0.2121\n",
      "Epoch [3/10], Batch [100/506], Loss: 0.4783\n",
      "Epoch [3/10], Batch [110/506], Loss: 0.4743\n",
      "Epoch [3/10], Batch [120/506], Loss: 0.4975\n",
      "Epoch [3/10], Batch [130/506], Loss: 0.2910\n",
      "Epoch [3/10], Batch [140/506], Loss: 0.5778\n",
      "Epoch [3/10], Batch [150/506], Loss: 0.6048\n",
      "Epoch [3/10], Batch [160/506], Loss: 0.2927\n",
      "Epoch [3/10], Batch [170/506], Loss: 0.2432\n",
      "Epoch [3/10], Batch [180/506], Loss: 0.4128\n",
      "Epoch [3/10], Batch [190/506], Loss: 0.2867\n",
      "Epoch [3/10], Batch [200/506], Loss: 0.3152\n",
      "Epoch [3/10], Batch [210/506], Loss: 0.2670\n",
      "Epoch [3/10], Batch [220/506], Loss: 0.4422\n",
      "Epoch [3/10], Batch [230/506], Loss: 0.5760\n",
      "Epoch [3/10], Batch [240/506], Loss: 0.1415\n",
      "Epoch [3/10], Batch [250/506], Loss: 0.3595\n",
      "Epoch [3/10], Batch [260/506], Loss: 0.2763\n",
      "Epoch [3/10], Batch [270/506], Loss: 0.2082\n",
      "Epoch [3/10], Batch [280/506], Loss: 0.1930\n",
      "Epoch [3/10], Batch [290/506], Loss: 0.2774\n",
      "Epoch [3/10], Batch [300/506], Loss: 0.3192\n",
      "Epoch [3/10], Batch [310/506], Loss: 0.4407\n",
      "Epoch [3/10], Batch [320/506], Loss: 0.1927\n",
      "Epoch [3/10], Batch [330/506], Loss: 0.3152\n",
      "Epoch [3/10], Batch [340/506], Loss: 0.4429\n",
      "Epoch [3/10], Batch [350/506], Loss: 0.3671\n",
      "Epoch [3/10], Batch [360/506], Loss: 0.3868\n",
      "Epoch [3/10], Batch [370/506], Loss: 0.4452\n",
      "Epoch [3/10], Batch [380/506], Loss: 0.1349\n",
      "Epoch [3/10], Batch [390/506], Loss: 0.5213\n",
      "Epoch [3/10], Batch [400/506], Loss: 0.1950\n",
      "Epoch [3/10], Batch [410/506], Loss: 0.3266\n",
      "Epoch [3/10], Batch [420/506], Loss: 0.2277\n",
      "Epoch [3/10], Batch [430/506], Loss: 0.1970\n",
      "Epoch [3/10], Batch [440/506], Loss: 0.3716\n",
      "Epoch [3/10], Batch [450/506], Loss: 0.3090\n",
      "Epoch [3/10], Batch [460/506], Loss: 0.4865\n",
      "Epoch [3/10], Batch [470/506], Loss: 0.2753\n",
      "Epoch [3/10], Batch [480/506], Loss: 0.5655\n",
      "Epoch [3/10], Batch [490/506], Loss: 0.3528\n",
      "Epoch [3/10], Batch [500/506], Loss: 0.1959\n",
      "Epoch 3/10 | Train Loss: 0.3443, Acc: 88.66% | Val Loss: 0.2927, Acc: 90.48%\n",
      "    Class battery: 95.21%\n",
      "    Class biological: 96.28%\n",
      "    Class cardboard: 87.43%\n",
      "    Class clothes: 97.91%\n",
      "    Class glass: 91.99%\n",
      "    Class metal: 84.58%\n",
      "    Class paper: 87.38%\n",
      "    Class plastic: 77.80%\n",
      "    Class shoes: 95.83%\n",
      "    Class trash: 71.71%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     battery       0.90      0.95      0.93       166\n",
      "  biological       0.90      0.96      0.93       188\n",
      "   cardboard       0.90      0.88      0.89       372\n",
      "     clothes       0.97      0.98      0.98      1099\n",
      "       glass       0.88      0.92      0.90       649\n",
      "       metal       0.79      0.85      0.81       226\n",
      "       paper       0.84      0.87      0.85       323\n",
      "     plastic       0.88      0.78      0.83       454\n",
      "       shoes       0.93      0.96      0.94       360\n",
      "       trash       0.88      0.72      0.79       205\n",
      "\n",
      "    accuracy                           0.90      4042\n",
      "   macro avg       0.89      0.89      0.89      4042\n",
      "weighted avg       0.90      0.90      0.90      4042\n",
      "\n",
      "Epoch [4/10], Batch [10/506], Loss: 0.4886\n",
      "Epoch [4/10], Batch [20/506], Loss: 0.2580\n",
      "Epoch [4/10], Batch [30/506], Loss: 0.2249\n",
      "Epoch [4/10], Batch [40/506], Loss: 0.3785\n",
      "Epoch [4/10], Batch [50/506], Loss: 0.1906\n",
      "Epoch [4/10], Batch [60/506], Loss: 0.1497\n",
      "Epoch [4/10], Batch [70/506], Loss: 0.2838\n",
      "Epoch [4/10], Batch [80/506], Loss: 0.3942\n",
      "Epoch [4/10], Batch [90/506], Loss: 0.5333\n",
      "Epoch [4/10], Batch [100/506], Loss: 0.1869\n",
      "Epoch [4/10], Batch [110/506], Loss: 0.4113\n",
      "Epoch [4/10], Batch [120/506], Loss: 0.3376\n",
      "Epoch [4/10], Batch [130/506], Loss: 0.1412\n",
      "Epoch [4/10], Batch [140/506], Loss: 0.1826\n",
      "Epoch [4/10], Batch [150/506], Loss: 0.4031\n",
      "Epoch [4/10], Batch [160/506], Loss: 0.2251\n",
      "Epoch [4/10], Batch [170/506], Loss: 0.2896\n",
      "Epoch [4/10], Batch [180/506], Loss: 0.2993\n",
      "Epoch [4/10], Batch [190/506], Loss: 0.1437\n",
      "Epoch [4/10], Batch [200/506], Loss: 0.2572\n",
      "Epoch [4/10], Batch [210/506], Loss: 0.1727\n",
      "Epoch [4/10], Batch [220/506], Loss: 0.4453\n",
      "Epoch [4/10], Batch [230/506], Loss: 0.3594\n",
      "Epoch [4/10], Batch [240/506], Loss: 0.4527\n",
      "Epoch [4/10], Batch [250/506], Loss: 0.2877\n",
      "Epoch [4/10], Batch [260/506], Loss: 0.3309\n",
      "Epoch [4/10], Batch [270/506], Loss: 0.4334\n",
      "Epoch [4/10], Batch [280/506], Loss: 0.4033\n",
      "Epoch [4/10], Batch [290/506], Loss: 0.5369\n",
      "Epoch [4/10], Batch [300/506], Loss: 0.4659\n",
      "Epoch [4/10], Batch [310/506], Loss: 0.0992\n",
      "Epoch [4/10], Batch [320/506], Loss: 0.2503\n",
      "Epoch [4/10], Batch [330/506], Loss: 0.6476\n",
      "Epoch [4/10], Batch [340/506], Loss: 0.3579\n",
      "Epoch [4/10], Batch [350/506], Loss: 0.2990\n",
      "Epoch [4/10], Batch [360/506], Loss: 0.2086\n",
      "Epoch [4/10], Batch [370/506], Loss: 0.3716\n",
      "Epoch [4/10], Batch [380/506], Loss: 0.2095\n",
      "Epoch [4/10], Batch [390/506], Loss: 0.3463\n",
      "Epoch [4/10], Batch [400/506], Loss: 0.1786\n",
      "Epoch [4/10], Batch [410/506], Loss: 0.2751\n",
      "Epoch [4/10], Batch [420/506], Loss: 0.5404\n",
      "Epoch [4/10], Batch [430/506], Loss: 0.3385\n",
      "Epoch [4/10], Batch [440/506], Loss: 0.7414\n",
      "Epoch [4/10], Batch [450/506], Loss: 0.2693\n",
      "Epoch [4/10], Batch [460/506], Loss: 0.2606\n",
      "Epoch [4/10], Batch [470/506], Loss: 0.3424\n",
      "Epoch [4/10], Batch [480/506], Loss: 0.5211\n",
      "Epoch [4/10], Batch [490/506], Loss: 0.3864\n",
      "Epoch [4/10], Batch [500/506], Loss: 0.2770\n",
      "Epoch 4/10 | Train Loss: 0.3326, Acc: 88.98% | Val Loss: 0.2883, Acc: 90.67%\n",
      "    Class battery: 95.21%\n",
      "    Class biological: 97.87%\n",
      "    Class cardboard: 86.90%\n",
      "    Class clothes: 97.55%\n",
      "    Class glass: 92.30%\n",
      "    Class metal: 81.50%\n",
      "    Class paper: 88.00%\n",
      "    Class plastic: 79.56%\n",
      "    Class shoes: 95.83%\n",
      "    Class trash: 74.15%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     battery       0.90      0.95      0.92       166\n",
      "  biological       0.91      0.98      0.94       188\n",
      "   cardboard       0.92      0.87      0.89       372\n",
      "     clothes       0.98      0.98      0.98      1099\n",
      "       glass       0.87      0.92      0.89       649\n",
      "       metal       0.82      0.81      0.82       226\n",
      "       paper       0.84      0.88      0.86       323\n",
      "     plastic       0.88      0.80      0.84       454\n",
      "       shoes       0.92      0.96      0.94       360\n",
      "       trash       0.88      0.74      0.80       205\n",
      "\n",
      "    accuracy                           0.91      4042\n",
      "   macro avg       0.89      0.89      0.89      4042\n",
      "weighted avg       0.91      0.91      0.91      4042\n",
      "\n",
      "Epoch [5/10], Batch [10/506], Loss: 0.3026\n",
      "Epoch [5/10], Batch [20/506], Loss: 0.5549\n",
      "Epoch [5/10], Batch [30/506], Loss: 0.6594\n",
      "Epoch [5/10], Batch [40/506], Loss: 0.3201\n",
      "Epoch [5/10], Batch [50/506], Loss: 0.4061\n",
      "Epoch [5/10], Batch [60/506], Loss: 0.3222\n",
      "Epoch [5/10], Batch [70/506], Loss: 0.3847\n",
      "Epoch [5/10], Batch [80/506], Loss: 0.4090\n",
      "Epoch [5/10], Batch [90/506], Loss: 0.5737\n",
      "Epoch [5/10], Batch [100/506], Loss: 0.4308\n",
      "Epoch [5/10], Batch [110/506], Loss: 0.4262\n",
      "Epoch [5/10], Batch [120/506], Loss: 0.2344\n",
      "Epoch [5/10], Batch [130/506], Loss: 0.1959\n",
      "Epoch [5/10], Batch [140/506], Loss: 0.4033\n",
      "Epoch [5/10], Batch [150/506], Loss: 0.1919\n",
      "Epoch [5/10], Batch [160/506], Loss: 0.1968\n",
      "Epoch [5/10], Batch [170/506], Loss: 0.3592\n",
      "Epoch [5/10], Batch [180/506], Loss: 0.5841\n",
      "Epoch [5/10], Batch [190/506], Loss: 0.0955\n",
      "Epoch [5/10], Batch [200/506], Loss: 0.2411\n",
      "Epoch [5/10], Batch [210/506], Loss: 0.1439\n",
      "Epoch [5/10], Batch [220/506], Loss: 0.2484\n",
      "Epoch [5/10], Batch [230/506], Loss: 0.3230\n",
      "Epoch [5/10], Batch [240/506], Loss: 0.5580\n",
      "Epoch [5/10], Batch [250/506], Loss: 0.3121\n",
      "Epoch [5/10], Batch [260/506], Loss: 0.4129\n",
      "Epoch [5/10], Batch [270/506], Loss: 0.4861\n",
      "Epoch [5/10], Batch [280/506], Loss: 0.2425\n",
      "Epoch [5/10], Batch [290/506], Loss: 0.4586\n",
      "Epoch [5/10], Batch [300/506], Loss: 0.3676\n",
      "Epoch [5/10], Batch [310/506], Loss: 0.2607\n",
      "Epoch [5/10], Batch [320/506], Loss: 0.4341\n",
      "Epoch [5/10], Batch [330/506], Loss: 0.2703\n",
      "Epoch [5/10], Batch [340/506], Loss: 0.1673\n",
      "Epoch [5/10], Batch [350/506], Loss: 0.1105\n",
      "Epoch [5/10], Batch [360/506], Loss: 0.5038\n",
      "Epoch [5/10], Batch [370/506], Loss: 0.3198\n",
      "Epoch [5/10], Batch [380/506], Loss: 0.4933\n",
      "Epoch [5/10], Batch [390/506], Loss: 0.3973\n",
      "Epoch [5/10], Batch [400/506], Loss: 0.4491\n",
      "Epoch [5/10], Batch [410/506], Loss: 0.3833\n",
      "Epoch [5/10], Batch [420/506], Loss: 0.3470\n",
      "Epoch [5/10], Batch [430/506], Loss: 0.3264\n",
      "Epoch [5/10], Batch [440/506], Loss: 0.2701\n",
      "Epoch [5/10], Batch [450/506], Loss: 0.7323\n",
      "Epoch [5/10], Batch [460/506], Loss: 0.4200\n",
      "Epoch [5/10], Batch [470/506], Loss: 0.2083\n",
      "Epoch [5/10], Batch [480/506], Loss: 0.4574\n",
      "Epoch [5/10], Batch [490/506], Loss: 0.3552\n",
      "Epoch [5/10], Batch [500/506], Loss: 0.5864\n",
      "Epoch 5/10 | Train Loss: 0.3289, Acc: 89.23% | Val Loss: 0.2851, Acc: 90.67%\n",
      "    Class battery: 97.01%\n",
      "    Class biological: 97.34%\n",
      "    Class cardboard: 87.70%\n",
      "    Class clothes: 97.55%\n",
      "    Class glass: 93.22%\n",
      "    Class metal: 81.94%\n",
      "    Class paper: 87.69%\n",
      "    Class plastic: 76.70%\n",
      "    Class shoes: 96.94%\n",
      "    Class trash: 73.17%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     battery       0.85      0.97      0.91       166\n",
      "  biological       0.91      0.97      0.94       188\n",
      "   cardboard       0.91      0.88      0.89       372\n",
      "     clothes       0.98      0.98      0.98      1099\n",
      "       glass       0.87      0.93      0.90       649\n",
      "       metal       0.81      0.82      0.82       226\n",
      "       paper       0.83      0.88      0.85       323\n",
      "     plastic       0.89      0.77      0.83       454\n",
      "       shoes       0.92      0.97      0.95       360\n",
      "       trash       0.90      0.73      0.81       205\n",
      "\n",
      "    accuracy                           0.91      4042\n",
      "   macro avg       0.89      0.89      0.89      4042\n",
      "weighted avg       0.91      0.91      0.91      4042\n",
      "\n",
      "Epoch [6/10], Batch [10/506], Loss: 0.2713\n",
      "Epoch [6/10], Batch [20/506], Loss: 0.1660\n",
      "Epoch [6/10], Batch [30/506], Loss: 0.2326\n",
      "Epoch [6/10], Batch [40/506], Loss: 0.2627\n",
      "Epoch [6/10], Batch [50/506], Loss: 0.4322\n",
      "Epoch [6/10], Batch [60/506], Loss: 0.3074\n",
      "Epoch [6/10], Batch [70/506], Loss: 0.4002\n",
      "Epoch [6/10], Batch [80/506], Loss: 0.3709\n",
      "Epoch [6/10], Batch [90/506], Loss: 0.1892\n",
      "Epoch [6/10], Batch [100/506], Loss: 0.1247\n",
      "Epoch [6/10], Batch [110/506], Loss: 0.5086\n",
      "Epoch [6/10], Batch [120/506], Loss: 0.1248\n",
      "Epoch [6/10], Batch [130/506], Loss: 0.2204\n",
      "Epoch [6/10], Batch [140/506], Loss: 0.2207\n",
      "Epoch [6/10], Batch [150/506], Loss: 0.3025\n",
      "Epoch [6/10], Batch [160/506], Loss: 0.3552\n",
      "Epoch [6/10], Batch [170/506], Loss: 0.6191\n",
      "Epoch [6/10], Batch [180/506], Loss: 0.4523\n",
      "Epoch [6/10], Batch [190/506], Loss: 0.1296\n",
      "Epoch [6/10], Batch [200/506], Loss: 0.3974\n",
      "Epoch [6/10], Batch [210/506], Loss: 0.7689\n",
      "Epoch [6/10], Batch [220/506], Loss: 0.3333\n",
      "Epoch [6/10], Batch [230/506], Loss: 0.3349\n",
      "Epoch [6/10], Batch [240/506], Loss: 0.2502\n",
      "Epoch [6/10], Batch [250/506], Loss: 0.3327\n",
      "Epoch [6/10], Batch [260/506], Loss: 0.2398\n",
      "Epoch [6/10], Batch [270/506], Loss: 0.3927\n",
      "Epoch [6/10], Batch [280/506], Loss: 0.2103\n",
      "Epoch [6/10], Batch [290/506], Loss: 0.2950\n",
      "Epoch [6/10], Batch [300/506], Loss: 0.4798\n",
      "Epoch [6/10], Batch [310/506], Loss: 0.1110\n",
      "Epoch [6/10], Batch [320/506], Loss: 0.1971\n",
      "Epoch [6/10], Batch [330/506], Loss: 0.3566\n",
      "Epoch [6/10], Batch [340/506], Loss: 0.3105\n",
      "Epoch [6/10], Batch [350/506], Loss: 0.3887\n",
      "Epoch [6/10], Batch [360/506], Loss: 0.2224\n",
      "Epoch [6/10], Batch [370/506], Loss: 0.4755\n",
      "Epoch [6/10], Batch [380/506], Loss: 0.4313\n",
      "Epoch [6/10], Batch [390/506], Loss: 0.4893\n",
      "Epoch [6/10], Batch [400/506], Loss: 0.3088\n",
      "Epoch [6/10], Batch [410/506], Loss: 0.4767\n",
      "Epoch [6/10], Batch [420/506], Loss: 0.1978\n",
      "Epoch [6/10], Batch [430/506], Loss: 0.1785\n",
      "Epoch [6/10], Batch [440/506], Loss: 0.3163\n",
      "Epoch [6/10], Batch [450/506], Loss: 0.2810\n",
      "Epoch [6/10], Batch [460/506], Loss: 0.4105\n",
      "Epoch [6/10], Batch [470/506], Loss: 0.2349\n",
      "Epoch [6/10], Batch [480/506], Loss: 0.2463\n",
      "Epoch [6/10], Batch [490/506], Loss: 0.3112\n",
      "Epoch [6/10], Batch [500/506], Loss: 0.1087\n",
      "Epoch 6/10 | Train Loss: 0.3209, Acc: 89.37% | Val Loss: 0.2785, Acc: 90.80%\n",
      "    Class battery: 95.81%\n",
      "    Class biological: 97.87%\n",
      "    Class cardboard: 87.70%\n",
      "    Class clothes: 97.46%\n",
      "    Class glass: 92.14%\n",
      "    Class metal: 83.26%\n",
      "    Class paper: 88.31%\n",
      "    Class plastic: 78.68%\n",
      "    Class shoes: 96.11%\n",
      "    Class trash: 74.63%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     battery       0.88      0.96      0.92       166\n",
      "  biological       0.91      0.98      0.94       188\n",
      "   cardboard       0.91      0.88      0.89       372\n",
      "     clothes       0.98      0.98      0.98      1099\n",
      "       glass       0.88      0.92      0.90       649\n",
      "       metal       0.81      0.83      0.82       226\n",
      "       paper       0.83      0.88      0.85       323\n",
      "     plastic       0.89      0.79      0.84       454\n",
      "       shoes       0.93      0.96      0.94       360\n",
      "       trash       0.88      0.75      0.81       205\n",
      "\n",
      "    accuracy                           0.91      4042\n",
      "   macro avg       0.89      0.89      0.89      4042\n",
      "weighted avg       0.91      0.91      0.91      4042\n",
      "\n",
      "Epoch [7/10], Batch [10/506], Loss: 0.2245\n",
      "Epoch [7/10], Batch [20/506], Loss: 0.2812\n",
      "Epoch [7/10], Batch [30/506], Loss: 0.2389\n",
      "Epoch [7/10], Batch [40/506], Loss: 0.3356\n",
      "Epoch [7/10], Batch [50/506], Loss: 0.2752\n",
      "Epoch [7/10], Batch [60/506], Loss: 0.3436\n",
      "Epoch [7/10], Batch [70/506], Loss: 0.4455\n",
      "Epoch [7/10], Batch [80/506], Loss: 0.4469\n",
      "Epoch [7/10], Batch [90/506], Loss: 0.2144\n",
      "Epoch [7/10], Batch [100/506], Loss: 0.3072\n",
      "Epoch [7/10], Batch [110/506], Loss: 0.2969\n",
      "Epoch [7/10], Batch [120/506], Loss: 0.2563\n",
      "Epoch [7/10], Batch [130/506], Loss: 0.3905\n",
      "Epoch [7/10], Batch [140/506], Loss: 0.2203\n",
      "Epoch [7/10], Batch [150/506], Loss: 0.1755\n",
      "Epoch [7/10], Batch [160/506], Loss: 0.2160\n",
      "Epoch [7/10], Batch [170/506], Loss: 0.3511\n",
      "Epoch [7/10], Batch [180/506], Loss: 0.1414\n",
      "Epoch [7/10], Batch [190/506], Loss: 0.1422\n",
      "Epoch [7/10], Batch [200/506], Loss: 0.2382\n",
      "Epoch [7/10], Batch [210/506], Loss: 0.2863\n",
      "Epoch [7/10], Batch [220/506], Loss: 0.3399\n",
      "Epoch [7/10], Batch [230/506], Loss: 0.1973\n",
      "Epoch [7/10], Batch [240/506], Loss: 0.1683\n",
      "Epoch [7/10], Batch [250/506], Loss: 0.3076\n",
      "Epoch [7/10], Batch [260/506], Loss: 0.3703\n",
      "Epoch [7/10], Batch [270/506], Loss: 0.2085\n",
      "Epoch [7/10], Batch [280/506], Loss: 0.2675\n",
      "Epoch [7/10], Batch [290/506], Loss: 0.4270\n",
      "Epoch [7/10], Batch [300/506], Loss: 0.1628\n",
      "Epoch [7/10], Batch [310/506], Loss: 0.3871\n",
      "Epoch [7/10], Batch [320/506], Loss: 0.1994\n",
      "Epoch [7/10], Batch [330/506], Loss: 0.1756\n",
      "Epoch [7/10], Batch [340/506], Loss: 0.1956\n",
      "Epoch [7/10], Batch [350/506], Loss: 0.2135\n",
      "Epoch [7/10], Batch [360/506], Loss: 0.4836\n",
      "Epoch [7/10], Batch [370/506], Loss: 0.2837\n",
      "Epoch [7/10], Batch [380/506], Loss: 0.2017\n",
      "Epoch [7/10], Batch [390/506], Loss: 0.4071\n",
      "Epoch [7/10], Batch [400/506], Loss: 0.2265\n",
      "Epoch [7/10], Batch [410/506], Loss: 0.1568\n",
      "Epoch [7/10], Batch [420/506], Loss: 0.2632\n",
      "Epoch [7/10], Batch [430/506], Loss: 0.1726\n",
      "Epoch [7/10], Batch [440/506], Loss: 0.1930\n",
      "Epoch [7/10], Batch [450/506], Loss: 0.1308\n",
      "Epoch [7/10], Batch [460/506], Loss: 0.1979\n",
      "Epoch [7/10], Batch [470/506], Loss: 0.4123\n",
      "Epoch [7/10], Batch [480/506], Loss: 0.1567\n",
      "Epoch [7/10], Batch [490/506], Loss: 0.1786\n",
      "Epoch [7/10], Batch [500/506], Loss: 0.1566\n",
      "Epoch 7/10 | Train Loss: 0.3183, Acc: 89.56% | Val Loss: 0.2778, Acc: 91.04%\n",
      "    Class battery: 96.41%\n",
      "    Class biological: 97.87%\n",
      "    Class cardboard: 87.97%\n",
      "    Class clothes: 97.55%\n",
      "    Class glass: 92.91%\n",
      "    Class metal: 83.26%\n",
      "    Class paper: 88.31%\n",
      "    Class plastic: 78.68%\n",
      "    Class shoes: 96.11%\n",
      "    Class trash: 75.61%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     battery       0.90      0.96      0.93       166\n",
      "  biological       0.90      0.98      0.94       188\n",
      "   cardboard       0.91      0.88      0.89       372\n",
      "     clothes       0.98      0.98      0.98      1099\n",
      "       glass       0.88      0.93      0.90       649\n",
      "       metal       0.81      0.83      0.82       226\n",
      "       paper       0.83      0.88      0.86       323\n",
      "     plastic       0.90      0.79      0.84       454\n",
      "       shoes       0.93      0.96      0.95       360\n",
      "       trash       0.91      0.76      0.82       205\n",
      "\n",
      "    accuracy                           0.91      4042\n",
      "   macro avg       0.89      0.89      0.89      4042\n",
      "weighted avg       0.91      0.91      0.91      4042\n",
      "\n",
      "Epoch [8/10], Batch [10/506], Loss: 0.3465\n",
      "Epoch [8/10], Batch [20/506], Loss: 0.2625\n",
      "Epoch [8/10], Batch [30/506], Loss: 0.3931\n",
      "Epoch [8/10], Batch [40/506], Loss: 0.1769\n",
      "Epoch [8/10], Batch [50/506], Loss: 0.4638\n",
      "Epoch [8/10], Batch [60/506], Loss: 0.2578\n",
      "Epoch [8/10], Batch [70/506], Loss: 0.6291\n",
      "Epoch [8/10], Batch [80/506], Loss: 0.4621\n",
      "Epoch [8/10], Batch [90/506], Loss: 0.2305\n",
      "Epoch [8/10], Batch [100/506], Loss: 0.2137\n",
      "Epoch [8/10], Batch [110/506], Loss: 0.2092\n",
      "Epoch [8/10], Batch [120/506], Loss: 0.2370\n",
      "Epoch [8/10], Batch [130/506], Loss: 0.2295\n",
      "Epoch [8/10], Batch [140/506], Loss: 0.2724\n",
      "Epoch [8/10], Batch [150/506], Loss: 0.2699\n",
      "Epoch [8/10], Batch [160/506], Loss: 0.3118\n",
      "Epoch [8/10], Batch [170/506], Loss: 0.3166\n",
      "Epoch [8/10], Batch [180/506], Loss: 0.3184\n",
      "Epoch [8/10], Batch [190/506], Loss: 0.4915\n",
      "Epoch [8/10], Batch [200/506], Loss: 0.3601\n",
      "Epoch [8/10], Batch [210/506], Loss: 0.3773\n",
      "Epoch [8/10], Batch [220/506], Loss: 0.2617\n",
      "Epoch [8/10], Batch [230/506], Loss: 0.7928\n",
      "Epoch [8/10], Batch [240/506], Loss: 0.4677\n",
      "Epoch [8/10], Batch [250/506], Loss: 0.2852\n",
      "Epoch [8/10], Batch [260/506], Loss: 0.4952\n",
      "Epoch [8/10], Batch [270/506], Loss: 0.3137\n",
      "Epoch [8/10], Batch [280/506], Loss: 0.2417\n",
      "Epoch [8/10], Batch [290/506], Loss: 0.1741\n",
      "Epoch [8/10], Batch [300/506], Loss: 0.3010\n",
      "Epoch [8/10], Batch [310/506], Loss: 0.1838\n",
      "Epoch [8/10], Batch [320/506], Loss: 0.2723\n",
      "Epoch [8/10], Batch [330/506], Loss: 0.4977\n",
      "Epoch [8/10], Batch [340/506], Loss: 0.3047\n",
      "Epoch [8/10], Batch [350/506], Loss: 0.1410\n",
      "Epoch [8/10], Batch [360/506], Loss: 0.3378\n",
      "Epoch [8/10], Batch [370/506], Loss: 0.4265\n",
      "Epoch [8/10], Batch [380/506], Loss: 0.4719\n",
      "Epoch [8/10], Batch [390/506], Loss: 0.1444\n",
      "Epoch [8/10], Batch [400/506], Loss: 0.4030\n",
      "Epoch [8/10], Batch [410/506], Loss: 0.2320\n",
      "Epoch [8/10], Batch [420/506], Loss: 0.1290\n",
      "Epoch [8/10], Batch [430/506], Loss: 0.4121\n",
      "Epoch [8/10], Batch [440/506], Loss: 0.4095\n",
      "Epoch [8/10], Batch [450/506], Loss: 0.3418\n",
      "Epoch [8/10], Batch [460/506], Loss: 0.1714\n",
      "Epoch [8/10], Batch [470/506], Loss: 0.3356\n",
      "Epoch [8/10], Batch [480/506], Loss: 0.2098\n",
      "Epoch [8/10], Batch [490/506], Loss: 0.0902\n",
      "Epoch [8/10], Batch [500/506], Loss: 0.2042\n",
      "Epoch 8/10 | Train Loss: 0.3153, Acc: 89.36% | Val Loss: 0.2779, Acc: 90.90%\n",
      "    Class battery: 96.41%\n",
      "    Class biological: 97.87%\n",
      "    Class cardboard: 88.24%\n",
      "    Class clothes: 97.28%\n",
      "    Class glass: 93.07%\n",
      "    Class metal: 82.82%\n",
      "    Class paper: 88.00%\n",
      "    Class plastic: 78.68%\n",
      "    Class shoes: 95.56%\n",
      "    Class trash: 75.12%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     battery       0.89      0.96      0.92       166\n",
      "  biological       0.91      0.98      0.94       188\n",
      "   cardboard       0.90      0.88      0.89       372\n",
      "     clothes       0.98      0.97      0.98      1099\n",
      "       glass       0.87      0.93      0.90       649\n",
      "       metal       0.82      0.83      0.82       226\n",
      "       paper       0.84      0.88      0.86       323\n",
      "     plastic       0.89      0.79      0.84       454\n",
      "       shoes       0.93      0.96      0.94       360\n",
      "       trash       0.91      0.75      0.82       205\n",
      "\n",
      "    accuracy                           0.91      4042\n",
      "   macro avg       0.89      0.89      0.89      4042\n",
      "weighted avg       0.91      0.91      0.91      4042\n",
      "\n",
      "Epoch [9/10], Batch [10/506], Loss: 0.0799\n",
      "Epoch [9/10], Batch [20/506], Loss: 0.3854\n",
      "Epoch [9/10], Batch [30/506], Loss: 0.1814\n",
      "Epoch [9/10], Batch [40/506], Loss: 0.2500\n",
      "Epoch [9/10], Batch [50/506], Loss: 0.4842\n",
      "Epoch [9/10], Batch [60/506], Loss: 0.4085\n",
      "Epoch [9/10], Batch [70/506], Loss: 0.2359\n",
      "Epoch [9/10], Batch [80/506], Loss: 0.4357\n",
      "Epoch [9/10], Batch [90/506], Loss: 0.2742\n",
      "Epoch [9/10], Batch [100/506], Loss: 0.3842\n",
      "Epoch [9/10], Batch [110/506], Loss: 0.3931\n",
      "Epoch [9/10], Batch [120/506], Loss: 0.4633\n",
      "Epoch [9/10], Batch [130/506], Loss: 0.3895\n",
      "Epoch [9/10], Batch [140/506], Loss: 0.2904\n",
      "Epoch [9/10], Batch [150/506], Loss: 0.1918\n",
      "Epoch [9/10], Batch [160/506], Loss: 0.3424\n",
      "Epoch [9/10], Batch [170/506], Loss: 0.2535\n",
      "Epoch [9/10], Batch [180/506], Loss: 0.2329\n",
      "Epoch [9/10], Batch [190/506], Loss: 0.4943\n",
      "Epoch [9/10], Batch [200/506], Loss: 0.3909\n",
      "Epoch [9/10], Batch [210/506], Loss: 0.4599\n",
      "Epoch [9/10], Batch [220/506], Loss: 0.3096\n",
      "Epoch [9/10], Batch [230/506], Loss: 0.2471\n",
      "Epoch [9/10], Batch [240/506], Loss: 0.7250\n",
      "Epoch [9/10], Batch [250/506], Loss: 0.2405\n",
      "Epoch [9/10], Batch [260/506], Loss: 0.3458\n",
      "Epoch [9/10], Batch [270/506], Loss: 0.3470\n",
      "Epoch [9/10], Batch [280/506], Loss: 0.1781\n",
      "Epoch [9/10], Batch [290/506], Loss: 0.1829\n",
      "Epoch [9/10], Batch [300/506], Loss: 0.3351\n",
      "Epoch [9/10], Batch [310/506], Loss: 0.3280\n",
      "Epoch [9/10], Batch [320/506], Loss: 0.1634\n",
      "Epoch [9/10], Batch [330/506], Loss: 0.1100\n",
      "Epoch [9/10], Batch [340/506], Loss: 0.2295\n",
      "Epoch [9/10], Batch [350/506], Loss: 0.1728\n",
      "Epoch [9/10], Batch [360/506], Loss: 0.2199\n",
      "Epoch [9/10], Batch [370/506], Loss: 0.3506\n",
      "Epoch [9/10], Batch [380/506], Loss: 0.2050\n",
      "Epoch [9/10], Batch [390/506], Loss: 0.5408\n",
      "Epoch [9/10], Batch [400/506], Loss: 0.1813\n",
      "Epoch [9/10], Batch [410/506], Loss: 0.3722\n",
      "Epoch [9/10], Batch [420/506], Loss: 0.3107\n",
      "Epoch [9/10], Batch [430/506], Loss: 0.1080\n",
      "Epoch [9/10], Batch [440/506], Loss: 0.2213\n",
      "Epoch [9/10], Batch [450/506], Loss: 0.2281\n",
      "Epoch [9/10], Batch [460/506], Loss: 0.3146\n",
      "Epoch [9/10], Batch [470/506], Loss: 0.3302\n",
      "Epoch [9/10], Batch [480/506], Loss: 0.5475\n",
      "Epoch [9/10], Batch [490/506], Loss: 0.1333\n",
      "Epoch [9/10], Batch [500/506], Loss: 0.2130\n",
      "Epoch 9/10 | Train Loss: 0.3127, Acc: 90.01% | Val Loss: 0.2761, Acc: 91.24%\n",
      "    Class battery: 95.21%\n",
      "    Class biological: 97.87%\n",
      "    Class cardboard: 88.77%\n",
      "    Class clothes: 97.55%\n",
      "    Class glass: 93.37%\n",
      "    Class metal: 84.58%\n",
      "    Class paper: 88.31%\n",
      "    Class plastic: 78.90%\n",
      "    Class shoes: 96.11%\n",
      "    Class trash: 75.61%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     battery       0.89      0.95      0.92       166\n",
      "  biological       0.90      0.98      0.94       188\n",
      "   cardboard       0.91      0.89      0.90       372\n",
      "     clothes       0.98      0.98      0.98      1099\n",
      "       glass       0.88      0.93      0.91       649\n",
      "       metal       0.83      0.85      0.84       226\n",
      "       paper       0.84      0.88      0.86       323\n",
      "     plastic       0.90      0.79      0.84       454\n",
      "       shoes       0.93      0.96      0.94       360\n",
      "       trash       0.90      0.76      0.82       205\n",
      "\n",
      "    accuracy                           0.91      4042\n",
      "   macro avg       0.90      0.90      0.89      4042\n",
      "weighted avg       0.91      0.91      0.91      4042\n",
      "\n",
      "Epoch [10/10], Batch [10/506], Loss: 0.2072\n",
      "Epoch [10/10], Batch [20/506], Loss: 0.2853\n",
      "Epoch [10/10], Batch [30/506], Loss: 0.1166\n",
      "Epoch [10/10], Batch [40/506], Loss: 0.1620\n",
      "Epoch [10/10], Batch [50/506], Loss: 0.2066\n",
      "Epoch [10/10], Batch [60/506], Loss: 0.3555\n",
      "Epoch [10/10], Batch [70/506], Loss: 0.3117\n",
      "Epoch [10/10], Batch [80/506], Loss: 0.1615\n",
      "Epoch [10/10], Batch [90/506], Loss: 0.4503\n",
      "Epoch [10/10], Batch [100/506], Loss: 0.1963\n",
      "Epoch [10/10], Batch [110/506], Loss: 0.2539\n",
      "Epoch [10/10], Batch [120/506], Loss: 0.1049\n",
      "Epoch [10/10], Batch [130/506], Loss: 0.1658\n",
      "Epoch [10/10], Batch [140/506], Loss: 0.3616\n",
      "Epoch [10/10], Batch [150/506], Loss: 0.1432\n",
      "Epoch [10/10], Batch [160/506], Loss: 0.4330\n",
      "Epoch [10/10], Batch [170/506], Loss: 0.2004\n",
      "Epoch [10/10], Batch [180/506], Loss: 0.2721\n",
      "Epoch [10/10], Batch [190/506], Loss: 0.2467\n",
      "Epoch [10/10], Batch [200/506], Loss: 0.1779\n",
      "Epoch [10/10], Batch [210/506], Loss: 0.3816\n",
      "Epoch [10/10], Batch [220/506], Loss: 0.3782\n",
      "Epoch [10/10], Batch [230/506], Loss: 0.2922\n",
      "Epoch [10/10], Batch [240/506], Loss: 0.2732\n",
      "Epoch [10/10], Batch [250/506], Loss: 0.3229\n",
      "Epoch [10/10], Batch [260/506], Loss: 0.2881\n",
      "Epoch [10/10], Batch [270/506], Loss: 0.1630\n",
      "Epoch [10/10], Batch [280/506], Loss: 0.3753\n",
      "Epoch [10/10], Batch [290/506], Loss: 0.3652\n",
      "Epoch [10/10], Batch [300/506], Loss: 0.1897\n",
      "Epoch [10/10], Batch [310/506], Loss: 0.1727\n",
      "Epoch [10/10], Batch [320/506], Loss: 0.2850\n",
      "Epoch [10/10], Batch [330/506], Loss: 0.1687\n",
      "Epoch [10/10], Batch [340/506], Loss: 0.2446\n",
      "Epoch [10/10], Batch [350/506], Loss: 0.2392\n",
      "Epoch [10/10], Batch [360/506], Loss: 0.3344\n",
      "Epoch [10/10], Batch [370/506], Loss: 0.2091\n",
      "Epoch [10/10], Batch [380/506], Loss: 0.3804\n",
      "Epoch [10/10], Batch [390/506], Loss: 0.5732\n",
      "Epoch [10/10], Batch [400/506], Loss: 0.3689\n",
      "Epoch [10/10], Batch [410/506], Loss: 0.2860\n",
      "Epoch [10/10], Batch [420/506], Loss: 0.4607\n",
      "Epoch [10/10], Batch [430/506], Loss: 0.3231\n",
      "Epoch [10/10], Batch [440/506], Loss: 0.1944\n",
      "Epoch [10/10], Batch [450/506], Loss: 0.0951\n",
      "Epoch [10/10], Batch [460/506], Loss: 0.5207\n",
      "Epoch [10/10], Batch [470/506], Loss: 0.4745\n",
      "Epoch [10/10], Batch [480/506], Loss: 0.3090\n",
      "Epoch [10/10], Batch [490/506], Loss: 0.2770\n",
      "Epoch [10/10], Batch [500/506], Loss: 0.1255\n",
      "Epoch 10/10 | Train Loss: 0.3088, Acc: 89.85% | Val Loss: 0.2759, Acc: 91.14%\n",
      "    Class battery: 96.41%\n",
      "    Class biological: 97.87%\n",
      "    Class cardboard: 88.77%\n",
      "    Class clothes: 97.37%\n",
      "    Class glass: 92.91%\n",
      "    Class metal: 84.58%\n",
      "    Class paper: 88.31%\n",
      "    Class plastic: 78.46%\n",
      "    Class shoes: 96.11%\n",
      "    Class trash: 76.10%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     battery       0.89      0.96      0.93       166\n",
      "  biological       0.90      0.98      0.94       188\n",
      "   cardboard       0.90      0.89      0.90       372\n",
      "     clothes       0.98      0.97      0.98      1099\n",
      "       glass       0.88      0.93      0.91       649\n",
      "       metal       0.81      0.85      0.83       226\n",
      "       paper       0.84      0.88      0.86       323\n",
      "     plastic       0.90      0.79      0.84       454\n",
      "       shoes       0.93      0.96      0.95       360\n",
      "       trash       0.90      0.76      0.83       205\n",
      "\n",
      "    accuracy                           0.91      4042\n",
      "   macro avg       0.89      0.90      0.89      4042\n",
      "weighted avg       0.91      0.91      0.91      4042\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Train\n",
    "train_model(model, num_epochs=10)\n",
    "\n",
    "#Save model\n",
    "torch.save(model.state_dict(), \"mobilenetv3_waste.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
